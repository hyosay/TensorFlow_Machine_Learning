# ğŸ“Œ TensorFlow_Machine_Learning
## ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ ì •ë¦¬ ë° ë¨¸ì‹ ëŸ¬ë‹ ê³µë¶€

## supervised learning
- image labeling
- Email spam filter
- Predicting exam score

supervised learningì—ëŠ” Training data setì´ ì¡´ì¬í•œë‹¤.

## Types of supervised learning

### - regression

|x(hours)|y(score)|
|---|---|
|10|90|
|9|80|
|3|50|
|2|30|

(Linear) Hypothesis(ê°€ì„¤)ì„ ì •í•œë‹¤.  
> ex) 7ì‹œê°„ ê³µë¶€í•˜ë©´ ì ìˆ˜ê°€ ì–¼ë§ˆê°€ ë‚˜ì˜¬ê¹Œ??   

#### Cost(Loss) function

`H(x) - y` ë¥¼ ì“°ì§€ì•Šê³  `(H(x) -y)^2`ë¥¼ ì“´ë‹¤.   
 ì™œëƒí•˜ë©´ ê°€ì„¤ì˜ ê°’ê³¼ ì‹¤ì œ ë°ì´í„°ì˜ ì°¨ê°€ ì–‘ìˆ˜ë‚˜ ìŒìˆ˜ ë¬´ì—‡ì´ ë‚˜ì˜¬ì§€ ëª¨ë¥´ë©° ì´ê²ƒë“¤ì´ í•©ì³ì ¸ì„œ ìƒì‡„ê°€ ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.
 > 3 + (-3) + 2 + (-2) = 0  <- Featureë¼ë¦¬ ìƒì‡„ê°€ ë˜ì–´ ì œëŒ€ë¡œëœ Featureë¥¼ ì°¾ì§€ ëª» í•  ìˆ˜ë„ ìˆë‹¤.
 
 <img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-09-06 á„‹á…©á„’á…® 6 41 18" src="https://user-images.githubusercontent.com/46950334/132196808-6b1dc77c-45bd-4b05-ba27-9a6f344533fb.png">
 
 `H(x) = Wx + b`   
 **point.1  *MINIMIZE COST(W, b)***
 
 ### linear regressionê³¼ cost(loss) functionì„ ì •ë¦¬í•´ë³´ìë©´ ê²°êµ­ H(x)ì˜ ê°’ì´ ë¬´ì—‡ì¸ì§€ hypothesisì´ ê¶ê¸ˆí• ë–„ minimize cost_funcì„ , ê°€ì¥ ê¸°ë³¸ì ìœ¼ë¡œ mse(ì˜¤ì°¨ì œê³±í•©)ì„ ì‚¬ìš©í•˜ë©° Wì˜ ê°’ê³¼ bì˜ ê°’ì„ ìµœì†Œí™” í•˜ê¸° ìœ„í•´ gradient descent algorithmì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.

### - binary classification   ex) yes/no, pass/ non-pass

|x(hours)|y(pass/fail)|
|---|---|
|10|P|
|9|P|
|3|F|
|2|F|

binary classificationë„ linear regressionìœ¼ë¡œ êµ¬ë¶„ì„ ì§€ì–´ë³´ë ¤í–ˆì§€ë§Œ xì˜ ê°’ì´ ì»¤ì§€ê±°ë‚˜,ì‘ì•„ì§€ë©´ ì†ì‹¤ë¹„ìš©ì´ í¬ê²Œ ì¡°ì •ë˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê³¡ì„  'S'ëª¨ì•™ì¸ logistic func(sigmoid func) ê³ ì•ˆí–ˆë‹¤.

logistic regressionì—ì„œëŠ” costë¥¼ mse, gredient descent algorithmì„ ì‚¬ìš©í•˜ë©´ local variable, global variableì´ ì¡´ì¬í•˜ì—¬ ì‚¬ìš©í•˜ê¸° ë¶ˆí¸í•˜ë‹¤. 
ê·¸ë˜ì„œ 
cost funcì„ mseë¡œ ì„ íƒí•˜ì—¬ gredient descentí˜•íƒœë¡œ ë‚˜íƒ€ë‚´ë©´ ì•ˆë˜ê³  cross entropyë¥¼ ì‚¬ìš©í•˜ì—¬ 2ì°¨ê³¡ì„ ì˜ í˜•íƒœë¡œ ë§Œë“  í›„ gredient descent algorithmì„ ì‚¬ìš©í•´ì•¼í•œë‹¤.
### - multi-label classification  ex)í•™ì 

|x(hours)|y(grade)|
|---|---|
|10|A|
|9|B|
|3|C|
|2|D|


## why activation function
##### ê·¸ì € ê°€ì¤‘ì¹˜ì™€ í¸í–¥ë§Œ êµ¬í•´ì„œ ë”í•˜ë©´ ì„ í˜•ì—°ê²°ì´ ë˜ëŠ”ë° ì´ê²ƒì„ ë¹„ì…˜í˜•ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ê¸° ìœ„í•´ì„œ activation functionì´ í•„ìš”í•˜ë‹¤.


### 1. sigmoid

* ### The range of the sigmoid function is from 0 to 1

### backpropagationì—ì„œëŠ” sigmoidê°€ ìœ„í—˜ í•  ìˆ˜ê°€ ìˆë‹¤.

ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ ì§„í–‰í• ë•Œ ê¸°ìš¸ê¸°ê°€ ì†ì‹¤ë˜ëŠ” ìƒí™©ì´ ë°œìƒ(**vanishing gradient problem**)


## 2. Relu(Rectified Linear Units

* ### It converges faster than sigmoid

#### imageëŠ” 0 ì—ì„œ 255ì˜ ê°’ì„ ê°€ì§, ê·¸ë˜ì„œ ë§ˆì´ë„ˆìŠ¤ì˜ ê°’ì„ ë°›ìœ¼ë©´ ì˜ë¯¸ê°€ ì—†ë‹¤ê³  íŒë‹¨, ì¦‰ 0ìœ¼ë¡œ ë°”ê¾¸ëŠ”ê²Œ ë§ë‹¤ê³  ìƒê°í•´ì„œ Reluê°€ ì²˜ë¦¬ê°€ ì˜ëœë‹¤ê³  ìƒê°í•¨ 

* ### Dhing ReLU / the linear form in the positive range

#### ìŒìˆ˜ì˜ ê°’ì´ ë§ì´ ë‚˜ì˜¤ë©´ íŒŒë¼ë¯¸í„° ì—…ë°ì´í„°ê°€ ì˜ ì•ˆë¨


ë”¥ëŸ¬ë‹, ë¨¸ì‹ ëŸ¬ë‹ ê³µë¶€ ë°©í–¥
1. ian goodfellowì˜ deep learningì„ í†µí•´ì„œ ì´ë¡ ì„ ë§ˆìŠ¤í„°í•˜ê¸°.

2. ë¨¸ì‹ ëŸ¬ë‹ì±…ì„ ë³‘í–‰í•˜ì—¬ ê³µë¶€í•˜ê¸°
 
## ë‚˜ì˜ ëª©í‘œëŠ” GANì„ ì—°êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.



ì°¸ê³  : https://gomguard.tistory.com/184?category=712467

 
